{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class a_opt(torch.optim.lr_scheduler.LRScheduler):\n",
    "    def __init__(self, x):\n",
    "        pass\n",
    "        \n",
    "model = torch.nn.LSTM(100, 100)\n",
    "c = a_opt(torch.optim.AdamW(model.parameters(), lr=0.1))\n",
    "isinstance(c, torch.optim.lr_scheduler.LRScheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class outProd(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, branch, trunk):\n",
    "        b, p = branch.shape[0], trunk.shape[0]\n",
    "        branch = branch.expand(b, p)\n",
    "        trunk = trunk.expand(b, p)\n",
    "        ctx.save_for_backward(branch, trunk)\n",
    "        return branch * trunk\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        branch, trunk = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        print(grad_input.shape, branch.shape, trunk.shape)\n",
    "        branch_grad = grad_input * trunk\n",
    "        trunk_grad = grad_input * branch\n",
    "        print(branch_grad.shape, trunk_grad.shape)\n",
    "        return branch_grad, trunk_grad\n",
    "\n",
    "\n",
    "x = torch.ones(10, 1)\n",
    "y = torch.linspace(0, 1, 10)[...,None].requires_grad_()\n",
    "a = outProd.apply(x,y)\n",
    "print(a)\n",
    "torch.autograd.grad(a, y, grad_outputs= torch.ones_like(a),retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(10, 1)\n",
    "y = torch.linspace(0, 1, 10)[None,...].requires_grad_()\n",
    "a = torch.outer(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(10, 10)\n",
    "b = torch.linspace(0, 1, 10)[None,...]\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class originDeepONetCard(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.branch = nn.Linear(1, 10)\n",
    "        self.trunk = nn.Linear(1, 10)\n",
    "    def forward(self, x):\n",
    "        branch = self.branch(x[0])\n",
    "        trunk = self.trunk(x[1])\n",
    "        out = torch.einsum(\"b i, p i -> b p\", branch, trunk)\n",
    "        return out\n",
    "    \n",
    "class DeepONetCard(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.branch = nn.Linear(1, 10)\n",
    "        self.trunk = nn.Linear(1, 10)\n",
    "    def forward(self, x):\n",
    "        p, b = x[1].shape[0], x[0].shape[0]\n",
    "        branch = self.branch(x[0])\n",
    "        trunk = self.trunk(x[1])\n",
    "        branch = branch.unsqueeze(1).expand(b, p, 10)\n",
    "        trunk = trunk.unsqueeze(0).expand(b, p, 10)\n",
    "        return (branch * trunk).sum(-1)\n",
    "    \n",
    "netA = originDeepONetCard()\n",
    "netB = DeepONetCard()\n",
    "a = torch.ones(10, 1)\n",
    "b = torch.linspace(0, 1, 10)[..., None].requires_grad_()\n",
    "x = (a, b)\n",
    "A = netA(x)\n",
    "B = netB(x)\n",
    "print(torch.autograd.grad(A, b, grad_outputs= torch.ones_like(A),retain_graph=True)[0])\n",
    "print(torch.autograd.grad(B, b, grad_outputs= torch.ones_like(B),retain_graph=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1])\n",
      "tensor([[[ 0.0016, -0.0471, -0.2148,  ..., -0.0615,  0.2788,  0.0963],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0016, -0.0471, -0.2148,  ..., -0.0615,  0.2788,  0.0963],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0016, -0.0471, -0.2148,  ..., -0.0615,  0.2788,  0.0963],\n",
      "         ...,\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0016, -0.0471, -0.2148,  ..., -0.0615,  0.2788,  0.0963],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0016, -0.0471, -0.2148,  ..., -0.0615,  0.2788,  0.0963],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0016, -0.0471, -0.2148,  ..., -0.0615,  0.2788,  0.0963]]])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0016, -0.0471, -0.2148, -0.2085,  0.1566, -0.3097,  0.3059, -0.0615,\n",
      "          0.2788,  0.0963]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0152], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "class net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.a(x)\n",
    "\n",
    "y = net()\n",
    "a = torch.randn(1, 10, requires_grad=True)\n",
    "a = a.expand(1000, 10)\n",
    "b = a.mean(0, keepdim=True)\n",
    "out = y(a)\n",
    "print(out.shape)\n",
    "grad_batched = torch.eye(1000)[...,None]\n",
    "grad = torch.autograd.grad(out, a, grad_outputs= grad_batched,retain_graph=True, is_grads_batched=True)[0]\n",
    "print(grad)\n",
    "for i in y.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "class DeepOnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.branch = nn.Linear(10, 10)\n",
    "        self.trunk = nn.Linear(1, 10)\n",
    "    \n",
    "    def forward(self, b, t, mode = \"a\"):\n",
    "        if mode == \"a\":\n",
    "            branch = self.branch(b)\n",
    "            trunk = self.trunk(t)\n",
    "            return torch.einsum(\"b i, p i -> b p\", branch, trunk)\n",
    "        else:\n",
    "            t = t.mean(0) # p, 1\n",
    "            branch = self.branch(b) # b, 10\n",
    "            trunk = self.trunk(t) # p, 10\n",
    "            return torch.einsum(\"b i, p i -> b p\", branch, trunk)\n",
    "\n",
    "batch = 600\n",
    "y = DeepOnet().cuda()\n",
    "trunk_inp = torch.randn(1000, 1, requires_grad=True, device=\"cuda\")\n",
    "branch_inp = torch.randn(batch, 10, requires_grad=True, device=\"cuda\")\n",
    "out = y(branch_inp, trunk_inp, mode=\"a\")\n",
    "\n",
    "# 1000 x 1000 x 1; b, p, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1487258112 1489715712\n",
      "torch.Size([600, 1000, 1])\n",
      "12.440793752670288\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "t = time.time()\n",
    "grad2 = []\n",
    "for i in out:\n",
    "    grad2.append(torch.autograd.grad(i, trunk_inp, grad_outputs= torch.ones_like(i),retain_graph=True, create_graph=True)[0])\n",
    "grad2 = torch.stack(grad2)\n",
    "print(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated())\n",
    "print(grad2.shape)\n",
    "#print(grad2)\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 600, 1000]) torch.Size([600, 1000])\n",
      "48334848 1486678016\n",
      "torch.Size([600, 1000, 1])\n",
      "0.022984743118286133\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "t = time.time()\n",
    "for i in range(1):\n",
    "    out = y(branch_inp, trunk_inp, mode=\"a\")\n",
    "\n",
    "grad_batched = torch.eye(batch, device = \"cuda\")[...,None].expand(batch, batch, 1000)\n",
    "print(grad_batched.shape, out.shape)\n",
    "grad3 = torch.autograd.grad(out, trunk_inp, grad_outputs= grad_batched,retain_graph=True, is_grads_batched= True, create_graph=True)[0]\n",
    "print(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated())\n",
    "print(grad3.shape)\n",
    "\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print((grad2 - grad3 > 1e-6).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
