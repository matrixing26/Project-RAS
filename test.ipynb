{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class a_opt(torch.optim.lr_scheduler.LRScheduler):\n",
    "    def __init__(self, x):\n",
    "        pass\n",
    "        \n",
    "model = torch.nn.LSTM(100, 100)\n",
    "c = a_opt(torch.optim.AdamW(model.parameters(), lr=0.1))\n",
    "isinstance(c, torch.optim.lr_scheduler.LRScheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class outProd(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, branch, trunk):\n",
    "        b, p = branch.shape[0], trunk.shape[0]\n",
    "        branch = branch.expand(b, p)\n",
    "        trunk = trunk.expand(b, p)\n",
    "        ctx.save_for_backward(branch, trunk)\n",
    "        return branch * trunk\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        branch, trunk = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        print(grad_input.shape, branch.shape, trunk.shape)\n",
    "        branch_grad = grad_input * trunk\n",
    "        trunk_grad = grad_input * branch\n",
    "        print(branch_grad.shape, trunk_grad.shape)\n",
    "        return branch_grad, trunk_grad\n",
    "\n",
    "\n",
    "x = torch.ones(10, 1)\n",
    "y = torch.linspace(0, 1, 10)[...,None].requires_grad_()\n",
    "a = outProd.apply(x,y)\n",
    "print(a)\n",
    "torch.autograd.grad(a, y, grad_outputs= torch.ones_like(a),retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(10, 1)\n",
    "y = torch.linspace(0, 1, 10)[None,...].requires_grad_()\n",
    "a = torch.outer(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(10, 10)\n",
    "b = torch.linspace(0, 1, 10)[None,...]\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class originDeepONetCard(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.branch = nn.Linear(1, 10)\n",
    "        self.trunk = nn.Linear(1, 10)\n",
    "    def forward(self, x):\n",
    "        branch = self.branch(x[0])\n",
    "        trunk = self.trunk(x[1])\n",
    "        out = torch.einsum(\"b i, p i -> b p\", branch, trunk)\n",
    "        return out\n",
    "    \n",
    "class DeepONetCard(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.branch = nn.Linear(1, 10)\n",
    "        self.trunk = nn.Linear(1, 10)\n",
    "    def forward(self, x):\n",
    "        p, b = x[1].shape[0], x[0].shape[0]\n",
    "        branch = self.branch(x[0])\n",
    "        trunk = self.trunk(x[1])\n",
    "        branch = branch.unsqueeze(1).expand(b, p, 10)\n",
    "        trunk = trunk.unsqueeze(0).expand(b, p, 10)\n",
    "        return (branch * trunk).sum(-1)\n",
    "    \n",
    "netA = originDeepONetCard()\n",
    "netB = DeepONetCard()\n",
    "a = torch.ones(10, 1)\n",
    "b = torch.linspace(0, 1, 10)[..., None].requires_grad_()\n",
    "x = (a, b)\n",
    "A = netA(x)\n",
    "B = netB(x)\n",
    "print(torch.autograd.grad(A, b, grad_outputs= torch.ones_like(A),retain_graph=True)[0])\n",
    "print(torch.autograd.grad(B, b, grad_outputs= torch.ones_like(B),retain_graph=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "class net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.a(x)\n",
    "\n",
    "y = net()\n",
    "a = torch.randn(1, 10, requires_grad=True)\n",
    "a = a.expand(1000, 10)\n",
    "b = a.mean(0, keepdim=True)\n",
    "out = y(a)\n",
    "print(out.shape)\n",
    "grad_batched = torch.eye(1000)[...,None]\n",
    "grad = torch.autograd.grad(out, a, grad_outputs= grad_batched,retain_graph=True, is_grads_batched=True)[0]\n",
    "print(grad)\n",
    "for i in y.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "class DeepOnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.branch = nn.Linear(10, 10)\n",
    "        self.trunk = nn.Sequential(nn.Linear(1, 100), nn.Linear(100,10))\n",
    "    \n",
    "    def forward(self, b, t, mode = \"a\"):\n",
    "        if mode == \"a\":\n",
    "            branch = self.branch(b)\n",
    "            trunk = self.trunk(t)\n",
    "            return torch.einsum(\"b i, p i -> b p\", branch, trunk)\n",
    "        else:\n",
    "            t = t.mean(0) # p, 1\n",
    "            branch = self.branch(b) # b, 10\n",
    "            trunk = self.trunk(t) # p, 10\n",
    "            return torch.einsum(\"b i, p i -> b p\", branch, trunk)\n",
    "\n",
    "batch = 600\n",
    "y = DeepOnet().cuda()\n",
    "trunk_inp = torch.randn(1000, 1, requires_grad=True, device=\"cuda\")\n",
    "branch_inp = torch.randn(batch, 10, requires_grad=True, device=\"cuda\")\n",
    "out = y(branch_inp, trunk_inp, mode=\"a\")\n",
    "\n",
    "# 1000 x 1000 x 1; b, p, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "t = time.time()\n",
    "grad2 = []\n",
    "for i in out:\n",
    "    grad2.append(torch.autograd.grad(i, trunk_inp, grad_outputs= torch.ones_like(i),retain_graph=True, create_graph=True)[0])\n",
    "grad2 = torch.stack(grad2)\n",
    "print(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated())\n",
    "print(grad2.shape)\n",
    "#print(grad2)\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "t = time.time()\n",
    "for i in range(1):\n",
    "    out = y(branch_inp, trunk_inp, mode=\"a\")\n",
    "\n",
    "grad_batched = torch.eye(batch, device = \"cuda\")[...,None].expand(batch, batch, 1000)\n",
    "print(grad_batched.shape, out.shape)\n",
    "grad3 = torch.autograd.grad(out, trunk_inp, grad_outputs= grad_batched,retain_graph=True, is_grads_batched= True, create_graph=True)[0]\n",
    "print(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated())\n",
    "print(grad3.shape)\n",
    "\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import deepxde.deepxde as dde\n",
    "import time\n",
    "\n",
    "net = dde.nn.pytorch.DeepONetCartesianProd([101, 100, 100], [2, 100, 100], \"gelu\", \"Glorot normal\")\n",
    "\n",
    "batch = 50\n",
    "branch = torch.randn(batch, 101)\n",
    "trunk = torch.randn(10000, 2).requires_grad_()\n",
    "\n",
    "result = net((branch, trunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "grad2 = []\n",
    "for i in result:\n",
    "    grad2.append(torch.autograd.grad(i, trunk, grad_outputs= torch.ones_like(i),retain_graph=True, create_graph=True)[0])\n",
    "grad2 = torch.stack(grad2)\n",
    "print(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated())\n",
    "print(grad2.shape)\n",
    "#print(grad2)\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "grad_batched = torch.eye(batch, device = \"cuda\")[...,None].expand(batch, batch, 10000)\n",
    "grad3 = torch.autograd.grad(result, trunk, grad_outputs= grad_batched,retain_graph=True, is_grads_batched= True, create_graph=True)[0]\n",
    "print(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated())\n",
    "print(grad3.shape)\n",
    "\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((grad2 - grad3 > 1e-6).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepxde.deepxde as dde\n",
    "import numpy as np\n",
    "f = dde.data.GRF(length_scale=0.1)\n",
    "fea = f.random(1)\n",
    "func = f.eval_batch(fea, np.linspace(0, 1, 101)[:, None])\n",
    "func = np.ones((1, 101))\n",
    "from datasets.solver import advection_solver\n",
    "xt, u = advection_solver(func)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig , (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.scatter(np.linspace(0, 1, 101), func[0])\n",
    "print(u[0])\n",
    "xt2 = xt.reshape(-1, 2)\n",
    "u2 = u.reshape(-1, 1)\n",
    "ax2.scatter(xt2[:, 0], xt2[:, 1], c = u2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "stat = torch.load(\"./results/adr_H1_norm_GRF_norm_19.pth\")\n",
    "print(stat.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = torch.load(\"./results/adr_pial_300000.pth\")\n",
    "print(stat.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stat['trunk.linears.0.weight'].shape)\n",
    "print(stat['branch.linears.0.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepxde.deepxde as dde\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import solver\n",
    "\n",
    "def periodic(x):\n",
    "    # print( \"shape\",torch.sin(x[:, 0] * 2 * np.pi).shape)\n",
    "    return torch.cat((torch.cos(x[:, 0] * 2 * np.pi).reshape(-1, 1), torch.sin(x[:, 0] * 2 * np.pi).reshape(-1, 1),\n",
    "                      torch.cos(2 * x[:, 0] * 2 * np.pi).reshape(-1, 1), torch.sin(2 * x[:, 0] * 2 * np.pi).reshape(-1, 1), x[:, 1].reshape(-1, 1)), 1)\n",
    "\n",
    "fsp = dde.data.GRF(length_scale = 0.05)\n",
    "fea = fsp.random(1)\n",
    "vx = fsp.eval_batch(fea, np.linspace(0, 1, 100))\n",
    "\n",
    "net = dde.nn.DeepONetCartesianProd(\n",
    "    [100, 100, 100],\n",
    "    [5, 100, 100, 100],\n",
    "    \"gelu\",\n",
    "    \"Glorot normal\",\n",
    ")\n",
    "\n",
    "net.apply_feature_transform(periodic)\n",
    "\n",
    "net.load_state_dict(stat, strict = False)\n",
    "print(vx.shape)\n",
    "xt, u = solver.diffusion_reaction_solver(vx[0], Nx = 100, Nt = 100)\n",
    "print(xt.shape)\n",
    "print(u.shape)\n",
    "\n",
    "\n",
    "\n",
    "geom = dde.geometry.Interval(0, 1)\n",
    "timedomain = dde.geometry.TimeDomain(0, 1)\n",
    "geomtime = dde.geometry.GeometryXTime(geom, timedomain)\n",
    "\n",
    "xt_uniform = xt.reshape(-1, 2)\n",
    "#xt_uniform = geomtime.uniform_points(10000)\n",
    "inputs = (torch.as_tensor(vx), torch.as_tensor(xt_uniform).float())\n",
    "# print(inputs[0], inputs[1])\n",
    "up = net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u.shape, up.shape)\n",
    "u_p = up[0].detach().cpu().numpy()\n",
    "u_t = u.flatten()\n",
    "print(u_p.shape, u_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 5))\n",
    "ax1.scatter(np.linspace(0, 1, 100), vx[0])\n",
    "ax2.scatter(x = xt_uniform[:, 0], y = xt_uniform[:, 1], c = u_t)\n",
    "ax3.scatter(x = xt_uniform[:, 0], y = xt_uniform[:, 1], c = u_p)\n",
    "ax1.set_title(\"vx\")\n",
    "ax1.set_xlim(0, 1)\n",
    "ax2.set_aspect(\"equal\")\n",
    "ax3.set_aspect(\"equal\")\n",
    "ax4.set_aspect(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.test_model import normONet\n",
    "import torch\n",
    "from torch import nn\n",
    "t = torch.randn(10000, 2)\n",
    "b = torch.randn(100, 101)\n",
    "\n",
    "net = normONet()\n",
    "\n",
    "net((t, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
