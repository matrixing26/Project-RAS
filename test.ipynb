{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class a_opt(torch.optim.lr_scheduler.LRScheduler):\n",
    "    def __init__(self, x):\n",
    "        pass\n",
    "        \n",
    "model = torch.nn.LSTM(100, 100)\n",
    "c = a_opt(torch.optim.AdamW(model.parameters(), lr=0.1))\n",
    "isinstance(c, torch.optim.lr_scheduler.LRScheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class outProd(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, branch, trunk):\n",
    "        b, p = branch.shape[0], trunk.shape[0]\n",
    "        branch = branch.expand(b, p)\n",
    "        trunk = trunk.expand(b, p)\n",
    "        ctx.save_for_backward(branch, trunk)\n",
    "        return branch * trunk\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        branch, trunk = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        print(grad_input.shape, branch.shape, trunk.shape)\n",
    "        branch_grad = grad_input * trunk\n",
    "        trunk_grad = grad_input * branch\n",
    "        print(branch_grad.shape, trunk_grad.shape)\n",
    "        return branch_grad, trunk_grad\n",
    "\n",
    "\n",
    "x = torch.ones(10, 1)\n",
    "y = torch.linspace(0, 1, 10)[...,None].requires_grad_()\n",
    "a = outProd.apply(x,y)\n",
    "print(a)\n",
    "torch.autograd.grad(a, y, grad_outputs= torch.ones_like(a),retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(10, 1)\n",
    "y = torch.linspace(0, 1, 10)[None,...].requires_grad_()\n",
    "a = torch.outer(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(10, 10)\n",
    "b = torch.linspace(0, 1, 10)[None,...]\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class originDeepONetCard(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.branch = nn.Linear(1, 10)\n",
    "        self.trunk = nn.Linear(1, 10)\n",
    "    def forward(self, x):\n",
    "        branch = self.branch(x[0])\n",
    "        trunk = self.trunk(x[1])\n",
    "        out = torch.einsum(\"b i, p i -> b p\", branch, trunk)\n",
    "        return out\n",
    "    \n",
    "class DeepONetCard(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.branch = nn.Linear(1, 10)\n",
    "        self.trunk = nn.Linear(1, 10)\n",
    "    def forward(self, x):\n",
    "        p, b = x[1].shape[0], x[0].shape[0]\n",
    "        branch = self.branch(x[0])\n",
    "        trunk = self.trunk(x[1])\n",
    "        branch = branch.unsqueeze(1).expand(b, p, 10)\n",
    "        trunk = trunk.unsqueeze(0).expand(b, p, 10)\n",
    "        return (branch * trunk).sum(-1)\n",
    "    \n",
    "netA = originDeepONetCard()\n",
    "netB = DeepONetCard()\n",
    "a = torch.ones(10, 1)\n",
    "b = torch.linspace(0, 1, 10)[..., None].requires_grad_()\n",
    "x = (a, b)\n",
    "A = netA(x)\n",
    "B = netB(x)\n",
    "print(torch.autograd.grad(A, b, grad_outputs= torch.ones_like(A),retain_graph=True)[0])\n",
    "print(torch.autograd.grad(B, b, grad_outputs= torch.ones_like(B),retain_graph=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "class net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.a(x)\n",
    "\n",
    "y = net()\n",
    "a = torch.randn(1, 10, requires_grad=True)\n",
    "a = a.expand(1000, 10)\n",
    "b = a.mean(0, keepdim=True)\n",
    "t_K = y(a)\n",
    "print(t_K.shape)\n",
    "grad_batched = torch.eye(1000)[...,None]\n",
    "grad = torch.autograd.grad(t_K, a, grad_outputs= grad_batched,retain_graph=True, is_grads_batched=True)[0]\n",
    "print(grad)\n",
    "for i in y.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "class DeepOnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.branch = nn.Linear(10, 10)\n",
    "        self.trunk = nn.Sequential(nn.Linear(1, 100), nn.Linear(100,10))\n",
    "    \n",
    "    def forward(self, b, t, mode = \"a\"):\n",
    "        if mode == \"a\":\n",
    "            branch = self.branch(b)\n",
    "            trunk = self.trunk(t)\n",
    "            return torch.einsum(\"b i, p i -> b p\", branch, trunk)\n",
    "        else:\n",
    "            t = t.mean(0) # p, 1\n",
    "            branch = self.branch(b) # b, 10\n",
    "            trunk = self.trunk(t) # p, 10\n",
    "            return torch.einsum(\"b i, p i -> b p\", branch, trunk)\n",
    "\n",
    "batch = 600\n",
    "y = DeepOnet().cuda()\n",
    "trunk_inp = torch.randn(1000, 1, requires_grad=True, device=\"cuda\")\n",
    "branch_inp = torch.randn(batch, 10, requires_grad=True, device=\"cuda\")\n",
    "t_K = y(branch_inp, trunk_inp, mode=\"a\")\n",
    "\n",
    "# 1000 x 1000 x 1; b, p, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "t = time.time()\n",
    "grad2 = []\n",
    "for i in t_K:\n",
    "    grad2.append(torch.autograd.grad(i, trunk_inp, grad_outputs= torch.ones_like(i),retain_graph=True, create_graph=True)[0])\n",
    "grad2 = torch.stack(grad2)\n",
    "print(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated())\n",
    "print(grad2.shape)\n",
    "#print(grad2)\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "t = time.time()\n",
    "for i in range(1):\n",
    "    t_K = y(branch_inp, trunk_inp, mode=\"a\")\n",
    "\n",
    "grad_batched = torch.eye(batch, device = \"cuda\")[...,None].expand(batch, batch, 1000)\n",
    "print(grad_batched.shape, t_K.shape)\n",
    "grad3 = torch.autograd.grad(t_K, trunk_inp, grad_outputs= grad_batched,retain_graph=True, is_grads_batched= True, create_graph=True)[0]\n",
    "print(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated())\n",
    "print(grad3.shape)\n",
    "\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import deepxde.deepxde as dde\n",
    "import time\n",
    "\n",
    "net = dde.nn.pytorch.DeepONetCartesianProd([101, 100, 100], [2, 100, 100], \"gelu\", \"Glorot normal\")\n",
    "\n",
    "batch = 50\n",
    "branch = torch.randn(batch, 101)\n",
    "trunk = torch.randn(10000, 2).requires_grad_()\n",
    "\n",
    "result = net((branch, trunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "grad2 = []\n",
    "for i in result:\n",
    "    grad2.append(torch.autograd.grad(i, trunk, grad_outputs= torch.ones_like(i),retain_graph=True, create_graph=True)[0])\n",
    "grad2 = torch.stack(grad2)\n",
    "print(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated())\n",
    "print(grad2.shape)\n",
    "#print(grad2)\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "grad_batched = torch.eye(batch, device = \"cuda\")[...,None].expand(batch, batch, 10000)\n",
    "grad3 = torch.autograd.grad(result, trunk, grad_outputs= grad_batched,retain_graph=True, is_grads_batched= True, create_graph=True)[0]\n",
    "print(torch.cuda.memory_allocated(), torch.cuda.max_memory_allocated())\n",
    "print(grad3.shape)\n",
    "\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((grad2 - grad3 > 1e-6).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepxde.deepxde as dde\n",
    "import numpy as np\n",
    "f = dde.data.GRF(length_scale=0.1)\n",
    "fea = f.random(1)\n",
    "func = f.eval_batch(fea, np.linspace(0, 1, 101)[:, None])\n",
    "func = np.ones((1, 101))\n",
    "from datasets.solver import advection_solver\n",
    "xt, u = advection_solver(func)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig , (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.scatter(np.linspace(0, 1, 101), func[0])\n",
    "print(u[0])\n",
    "xt2 = xt.reshape(-1, 2)\n",
    "u2 = u.reshape(-1, 1)\n",
    "ax2.scatter(xt2[:, 0], xt2[:, 1], c = u2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "stat = torch.load(\"./results/adr_H1_norm_GRF_norm_19.pth\")\n",
    "print(stat.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = torch.load(\"./results/adr_pial_300000.pth\")\n",
    "print(stat.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stat['trunk.linears.0.weight'].shape)\n",
    "print(stat['branch.linears.0.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepxde.deepxde as dde\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import solver\n",
    "\n",
    "def periodic(x):\n",
    "    # print( \"shape\",torch.sin(x[:, 0] * 2 * np.pi).shape)\n",
    "    return torch.cat((torch.cos(x[:, 0] * 2 * np.pi).reshape(-1, 1), torch.sin(x[:, 0] * 2 * np.pi).reshape(-1, 1),\n",
    "                      torch.cos(2 * x[:, 0] * 2 * np.pi).reshape(-1, 1), torch.sin(2 * x[:, 0] * 2 * np.pi).reshape(-1, 1), x[:, 1].reshape(-1, 1)), 1)\n",
    "\n",
    "fsp = dde.data.GRF(length_scale = 0.05)\n",
    "fea = fsp.random(1)\n",
    "vx = fsp.eval_batch(fea, np.linspace(0, 1, 100))\n",
    "\n",
    "net = dde.nn.DeepONetCartesianProd(\n",
    "    [100, 100, 100],\n",
    "    [5, 100, 100, 100],\n",
    "    \"gelu\",\n",
    "    \"Glorot normal\",\n",
    ")\n",
    "\n",
    "net.apply_feature_transform(periodic)\n",
    "\n",
    "net.load_state_dict(stat, strict = False)\n",
    "print(vx.shape)\n",
    "xt, u = solver.diffusion_reaction_solver(vx[0], Nx = 100, Nt = 100)\n",
    "print(xt.shape)\n",
    "print(u.shape)\n",
    "\n",
    "\n",
    "\n",
    "geom = dde.geometry.Interval(0, 1)\n",
    "timedomain = dde.geometry.TimeDomain(0, 1)\n",
    "geomtime = dde.geometry.GeometryXTime(geom, timedomain)\n",
    "\n",
    "xt_uniform = xt.reshape(-1, 2)\n",
    "#xt_uniform = geomtime.uniform_points(10000)\n",
    "inputs = (torch.as_tensor(vx), torch.as_tensor(xt_uniform).float())\n",
    "# print(inputs[0], inputs[1])\n",
    "up = net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u.shape, up.shape)\n",
    "u_p = up[0].detach().cpu().numpy()\n",
    "u_t = u.flatten()\n",
    "print(u_p.shape, u_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 5))\n",
    "ax1.scatter(np.linspace(0, 1, 100), vx[0])\n",
    "ax2.scatter(x = xt_uniform[:, 0], y = xt_uniform[:, 1], c = u_t)\n",
    "ax3.scatter(x = xt_uniform[:, 0], y = xt_uniform[:, 1], c = u_p)\n",
    "ax1.set_title(\"vx\")\n",
    "ax1.set_xlim(0, 1)\n",
    "ax2.set_aspect(\"equal\")\n",
    "ax3.set_aspect(\"equal\")\n",
    "ax4.set_aspect(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.test_model import normONet\n",
    "import torch\n",
    "from torch import nn\n",
    "t = torch.randn(10000, 2)\n",
    "b = torch.randn(100, 101)\n",
    "\n",
    "net = normONet()\n",
    "\n",
    "net((t, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def solve_CVC(xmin: float, xmax: float, tmin: float, tmax: float, v: np.ndarray, g, f, Nx: int,  Nt: int,  upsample: int = 5):\n",
    "    # Create grid\n",
    "    Mx, Mt = (Nx - 1) * upsample + 1, (Nt - 1) * upsample + 1\n",
    "    x = np.linspace(xmin, xmax, Nx)\n",
    "    t = np.linspace(tmin, tmax, Nt)\n",
    "    X = np.linspace(0, xmax, Mx)\n",
    "    T = np.linspace(0, tmax, Mt)\n",
    "    # print(X.shape, v.shape)\n",
    "    h = X[1] - X[0]\n",
    "    dt = T[1] - T[0]\n",
    "    lam = dt / h\n",
    "    \n",
    "    # Computer advection velocity\n",
    "    vn = np.interp(X, x, v.flatten())\n",
    "    \n",
    "    # Initialize solution and apply initial & boundary conditions\n",
    "    u = np.zeros((Mx, Mt))\n",
    "    u[0, :] = g(T)\n",
    "    u[:, 0] = f(X)\n",
    "    \n",
    "    # Compute finite difference operators\n",
    "    mid = (vn[:-1] + vn[1:]) / 2\n",
    "    k = (1 - mid * lam) / (1 + mid * lam)\n",
    "    K = np.eye(Mx - 1, k = 0)\n",
    "    K_temp = np.eye(Mx - 1, k = 0)\n",
    "    Trans = np.eye(Mx - 1, k = -1)\n",
    "    \n",
    "    tv = torch.from_numpy(k)\n",
    "    l = tv.shape[0]\n",
    "    t_K = torch.ones((l,l)).tril()\n",
    "    _ = [t_K[...,i:,:i].mul_(-tv[i]) for i in range(1, l)]\n",
    "    \n",
    "    def body_fn_x(i, carry):\n",
    "        K, K_temp = carry\n",
    "        K_temp = (-k[:, None] * (Trans @ K_temp))\n",
    "        K += K_temp\n",
    "        return K, K_temp\n",
    "    \n",
    "    for i in range(Mx - 2):\n",
    "        K, K_temp = body_fn_x(i, (K, K_temp))\n",
    "    \n",
    "    \n",
    "    D = np.diag(k) + np.eye(Mx - 1, k=-1)\n",
    "    t_D = torch.from_numpy(D).float()\n",
    "    tg = lambda t: (t * torch.pi).sin()\n",
    "    t = torch.linspace(0, 1, 501)\n",
    "    gt = tg(t)    \n",
    "    t_B = gt[:-1] - gt[1:] * tv[0]\n",
    "    \n",
    "    z = torch.zeros(500, 500)\n",
    "    z[:,0] = t_B\n",
    "    t_B = z\n",
    "    t = time.time()\n",
    "    outu = torch.empty((501, 501))\n",
    "    outu[0] = (torch.linspace(0, 1, 501) * torch.pi).sin()\n",
    "    outu[:,0] = (torch.linspace(0, 1, 501) * torch.pi / 2).sin()\n",
    "    viu = outu[1:]\n",
    "    print(\"K check:\", ((t_K.numpy() - K) < 1e-5).all())\n",
    "    print(\"D check:\", ((t_D.numpy() - D) < 1e-5).all())\n",
    "    for i in range(0, Mt - 1):\n",
    "        b = np.zeros(Mx - 1)\n",
    "        b[0] = g(i * dt) - k[0] * g((i + 1) * dt)\n",
    "        tb = t_B[i]\n",
    "        #print(t_B[i,0], b[0])\n",
    "        print(\"B check:\", ((b - tb.numpy()) < 1e-5).all())\n",
    "        buf = K @ (D @ u[1:, i] + b)\n",
    "        \n",
    "        ch = t_K @ (t_D @ viu[:, i] + t_B[i])\n",
    "        # print((buf - ch.numpy() < 1e-5))\n",
    "        u[1:, i + 1] = buf\n",
    "        viu[:, i + 1] = ch\n",
    "        print(\"U check:\", ((u[1:, i] - viu[:, i].numpy()) < 1e-5).all())\n",
    "        \n",
    "    UU = u[::upsample, ::upsample]\n",
    "    t_UU = outu[::upsample, ::upsample]\n",
    "    print(\"solution check:\", ((t_UU.numpy() - UU) < 1e-5).all())\n",
    "    \n",
    "    return x, t, UU\n",
    "\n",
    "v = np.random.randn(101)\n",
    "v = np.abs(v) + 0.5\n",
    "\n",
    "t = time.time()\n",
    "_, _, u = solve_CVC(0, 1, 0, 1, v, lambda t: np.sin(t * np.pi), lambda x: np.sin(x * np.pi / 2), 101, 101)\n",
    "print(u.shape)\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.874260425567627\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from typing import Callable\n",
    "from numpy.typing import ArrayLike\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class CVCSolver(nn.Module):\n",
    "    def __init__(self, xmin: float = 0, xmax: float = 1, tmin: float = 0, tmax: float = 1, Nx: int = 101, Nt: int = 101, upsample: int = 5):\n",
    "        super().__init__()\n",
    "        self.xmin = xmin\n",
    "        self.xmax = xmax\n",
    "        self.tmin = tmin\n",
    "        self.tmax = tmax\n",
    "        self.Nx = Nx\n",
    "        self.Nt = Nt\n",
    "        self.upsample = upsample\n",
    "        self.Mx = (Nx - 1) * upsample + 1\n",
    "        self.Mt = (Nt - 1) * upsample + 1\n",
    "        self.register_buffer(\"xgrid\", torch.linspace(xmin, xmax, Nx))\n",
    "        self.register_buffer(\"tgrid\", torch.linspace(tmin, tmax, Nt))\n",
    "        self.register_buffer(\"Xgrid\", torch.linspace(xmin, xmax, self.Mx))\n",
    "        self.register_buffer(\"Tgrid\", torch.linspace(tmin, tmax, self.Mt))\n",
    "        self.register_buffer(\"offdiag\", torch.ones(self.Mx - 2).diag(-1))\n",
    "        self.dx = self.Xgrid[1] - self.Xgrid[0]\n",
    "        self.dt = self.Tgrid[1] - self.Tgrid[0]\n",
    "        self.lam = self.dt / self.dx\n",
    "        self.register_buffer(\"tk\", torch.ones((self.Mx - 1,self.Mx - 1)).tril().unsqueeze(0))\n",
    "        \n",
    "    def forward(self, vxs: ArrayLike, g: Callable[[Tensor], Tensor] = lambda t: (t * torch.pi).sin(), f: Callable[[Tensor], Tensor] = lambda x: (x * torch.pi / 2).sin()):\n",
    "        \"\"\"\n",
    "        _summary_\n",
    "\n",
    "        Args:\n",
    "            vxs (ArrayLike): should be a batched array of shape B, N\n",
    "            g (_type_, optional): _description_. Defaults to lambdat:(t * torch.pi).sin().\n",
    "            f (_type_, optional): _description_. Defaults to lambdax:(x * torch.pi / 2).sin().\n",
    "        \"\"\"\n",
    "        vxs = torch.as_tensor(vxs, dtype = self.Xgrid.dtype, device = self.Xgrid.device)\n",
    "        vxs = interp_nd(vxs, self.Xgrid[:, None] * 2 - 1, mode = \"linear\")\n",
    "        u = torch.empty(vxs.shape[0], self.Mx, self.Mt, dtype = self.Xgrid.dtype, device = self.Xgrid.device)\n",
    "        u[:, 0] = g(self.Tgrid).unsqueeze(0).expand(u.shape[0], -1)\n",
    "        u[:, :, 0] = f(self.Xgrid).unsqueeze(0).expand(u.shape[0], -1)\n",
    "        \n",
    "        mid = (vxs[:, :-1] + vxs[:, 1:]) / 2\n",
    "        k = (1 - mid * self.lam) / (1 + mid * self.lam)\n",
    "        \n",
    "        t_K = self.tk.repeat(k.shape[0], 1, 1)\n",
    "        _ = [t_K[:, i:, :i].mul_(-k[:, (i,), None]) for i in range(1, k.shape[1])]\n",
    "        t_B = u[:, 0, :-1] - u[:, 0, 1:] * k[:, (0,)]\n",
    "        t_D = torch.diag_embed(k) + self.offdiag.unsqueeze(0)\n",
    "        # print(t_K.shape, t_D.shape, t_B.shape)\n",
    "        viu = u[:, 1:]\n",
    "        for i in range(0, self.Mt - 1):\n",
    "            buf = torch.einsum(\"bik,bk->bi\", t_D, viu[:, :, i])\n",
    "            buf[:, 0] += t_B[:, i]\n",
    "            buf = torch.einsum(\"bik,bk->bi\", t_K, buf)\n",
    "            viu[:, :, i + 1] = buf\n",
    "            \n",
    "        return u[:, ::self.upsample, ::self.upsample]\n",
    "        \n",
    "def solve_CVC(xmin: float, xmax: float, tmin: float, tmax: float, v: np.ndarray, g, f, Nx: int,  Nt: int,  upsample: int = 5):\n",
    "    # Create grid\n",
    "    Mx, Mt = (Nx - 1) * upsample + 1, (Nt - 1) * upsample + 1\n",
    "    x = np.linspace(xmin, xmax, Nx)\n",
    "    t = np.linspace(tmin, tmax, Nt)\n",
    "    X = np.linspace(0, xmax, Mx)\n",
    "    T = np.linspace(0, tmax, Mt)\n",
    "    # print(X.shape, v.shape)\n",
    "    h = X[1] - X[0]\n",
    "    dt = T[1] - T[0]\n",
    "    lam = dt / h\n",
    "    \n",
    "    # Computer advection velocity\n",
    "    vn = np.interp(X, x, v.flatten())\n",
    "    # print(vn)\n",
    "    # Initialize solution and apply initial & boundary conditions\n",
    "    u = np.zeros((Mx, Mt))\n",
    "    u[0, :] = g(T)\n",
    "    u[:, 0] = f(X)\n",
    "    \n",
    "    # Compute finite difference operators\n",
    "    mid = (vn[:-1] + vn[1:]) / 2\n",
    "    k = (1 - mid * lam) / (1 + mid * lam)\n",
    "    K = np.eye(Mx - 1, k = 0)\n",
    "    K_temp = np.eye(Mx - 1, k = 0)\n",
    "    Trans = np.eye(Mx - 1, k = -1)\n",
    "    \n",
    "    tv = torch.from_numpy(k)\n",
    "    l = tv.shape[0]\n",
    "    t_K = torch.ones((l,l)).tril()\n",
    "    _ = [t_K[...,i:,:i].mul_(-tv[i]) for i in range(1, l)]\n",
    "    \n",
    "    def body_fn_x(i, carry):\n",
    "        K, K_temp = carry\n",
    "        K_temp = (-k[:, None] * (Trans @ K_temp))\n",
    "        K += K_temp\n",
    "        return K, K_temp\n",
    "    \n",
    "    for i in range(Mx - 2):\n",
    "        K, K_temp = body_fn_x(i, (K, K_temp))\n",
    "    \n",
    "    \n",
    "    D = np.diag(k) + np.eye(Mx - 1, k=-1)\n",
    "    t_D = torch.from_numpy(D).float()\n",
    "    tg = lambda t: (t * torch.pi).sin()\n",
    "    t = torch.linspace(0, 1, 501)\n",
    "    gt = tg(t)\n",
    "    t_B = gt[:-1] - gt[1:] * tv[0]\n",
    "    \n",
    "    z = torch.zeros(500, 500)\n",
    "    z[:,0] = t_B\n",
    "    t_B = z\n",
    "    t = time.time()\n",
    "    outu = torch.empty((501, 501))\n",
    "    outu[0] = (torch.linspace(0, 1, 501) * torch.pi).sin()\n",
    "    outu[:,0] = (torch.linspace(0, 1, 501) * torch.pi / 2).sin()\n",
    "    viu = outu[1:]\n",
    "    #print(\"K check:\", ((t_K.numpy() - K) < 1e-5).all())\n",
    "    #print(\"D check:\", ((t_D.numpy() - D) < 1e-5).all())\n",
    "    for i in range(0, Mt - 1):\n",
    "        b = np.zeros(Mx - 1)\n",
    "        b[0] = g(i * dt) - k[0] * g((i + 1) * dt)\n",
    "        tb = t_B[i]\n",
    "        #print(t_B[i,0], b[0])\n",
    "        #print(\"B check:\", ((b - tb.numpy()) < 1e-5).all())\n",
    "        buf = K @ (D @ u[1:, i] + b)\n",
    "        \n",
    "        ch = t_K @ (t_D @ viu[:, i] + t_B[i])\n",
    "        # print((buf - ch.numpy() < 1e-5))\n",
    "        u[1:, i + 1] = buf\n",
    "        viu[:, i + 1] = ch\n",
    "        #print(\"U check:\", ((u[1:, i] - viu[:, i].numpy()) < 1e-5).all())\n",
    "        \n",
    "    UU = u[::upsample, ::upsample]\n",
    "    t_UU = outu[::upsample, ::upsample]\n",
    "    #print(\"solution check:\", ((t_UU.numpy() - UU) < 1e-5).all())\n",
    "    \n",
    "    return x, t, UU\n",
    "\n",
    "# Adapted from https://github.com/luo3300612/grid_sample1d, support 1D, 2D, 3D grid\n",
    "def grid_sample(input: Tensor, \n",
    "                grid: Tensor, \n",
    "                mode:str = \"bilinear\", \n",
    "                padding_mode:str = \"zeros\", \n",
    "                align_corners: bool | None = None\n",
    "                ) -> Tensor:\n",
    "    if grid.shape[-1] == 1:\n",
    "        assert mode in [\"nearest\", \"linear\"], \"1D grid only support nearest and linear mode\"\n",
    "        input = input.unsqueeze(-1)\n",
    "        grid = grid.unsqueeze(1)\n",
    "        grid = torch.cat([-torch.ones_like(grid), grid], dim=-1)\n",
    "        out_shape = [grid.shape[0], input.shape[1], grid.shape[2]]\n",
    "        return F.grid_sample(input, grid, \n",
    "                             mode= \"bilinear\" if mode == \"linear\" else mode, \n",
    "                             padding_mode=padding_mode, \n",
    "                             align_corners=align_corners).view(*out_shape)\n",
    "    else:\n",
    "        return F.grid_sample(input, grid, mode=mode, padding_mode=padding_mode, align_corners=align_corners)\n",
    "\n",
    "def interp_nd(grid: Tensor, \n",
    "              points: Tensor, \n",
    "              mode: str = \"linear\", \n",
    "              align_corners: bool = True, \n",
    "              padding_mode: str = \"border\"\n",
    "              ) -> Tensor:\n",
    "    \"\"\"\n",
    "    Using torch to do interpolation. Support 1D, 2D, 3D grid. And for grid inputs, points can be unbatched or batched, but for unbatched grid inputs, points must be unbatched.\n",
    "\n",
    "    Args:\n",
    "        grid (torch.Tensor): the input function, shape `B, (C,) D, (H,) (W,)` for batched input, `(C,) D, (H,) (W,)` for unbatched input, channel dimension is optional.\n",
    "        points (torch.Tensor): `B, N, dim` for batched input, `N, dim` for unbatched input. the points should be in the range of `[-1, 1]`.\n",
    "        mode (str, optional): the mode for interpolation. Defaults to \"linear\".\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: shape `(B,) (C,) N\n",
    "    \"\"\"\n",
    "    interp_dim = points.shape[-1]\n",
    "    is_point_batched = points.dim() == 3\n",
    "    is_grid_batched = grid.dim() > interp_dim\n",
    "    is_channelled = grid.dim() -1 == is_grid_batched + interp_dim\n",
    "    \n",
    "    if not is_channelled:\n",
    "        grid = grid.unsqueeze(int(is_grid_batched))\n",
    "        \n",
    "    if not is_grid_batched:\n",
    "        grid = grid.unsqueeze(0)\n",
    "    if not is_point_batched:\n",
    "        points = points.unsqueeze(0).expand(grid.shape[0], -1, -1)\n",
    "    \n",
    "    for _ in range(interp_dim - 1):\n",
    "        points = points.unsqueeze(-2)\n",
    "    \n",
    "    grid = grid_sample(grid, points, mode=mode, padding_mode=padding_mode, align_corners=align_corners)\n",
    "    for _ in range(interp_dim - 1):\n",
    "        grid = grid.squeeze(-1)\n",
    "    \n",
    "    if not is_grid_batched:\n",
    "        grid = grid.squeeze(0)\n",
    "    \n",
    "    if not is_channelled:\n",
    "        grid = grid.squeeze(int(is_grid_batched))\n",
    "    \n",
    "    return grid\n",
    "\n",
    "vs = np.random.randn(200, 101)\n",
    "vs = np.abs(vs) + 0.5\n",
    "\n",
    "table = []\n",
    "# t = time.time()\n",
    "# for v in vs:\n",
    "#     _, _, u = solve_CVC(0, 1, 0, 1, v, lambda t: np.sin(t * np.pi), lambda x: np.sin(x * np.pi / 2), 101, 101)\n",
    "#     table.append(u)\n",
    "# print(time.time() - t)\n",
    "\n",
    "# print(u)\n",
    "solver = CVCSolver().cuda()\n",
    "t = time.time()\n",
    "uT = solver(vs)\n",
    "print(time.time() - t)\n",
    "# for i, j in zip(table, uT.numpy()):\n",
    "#     print(np.allclose(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  3.14107591e-02  6.27905195e-02 ...  6.27905195e-02\n",
      "   3.14107591e-02  1.22464680e-16]\n",
      " [ 1.57073173e-02 -3.58328753e-04  2.88321883e-02 ...  9.66763020e-02\n",
      "   6.53649709e-02  3.39891324e-02]\n",
      " [ 3.14107591e-02  1.14895732e-02  4.47614887e-03 ...  1.19205161e-01\n",
      "   8.79595517e-02  5.66271365e-02]\n",
      " ...\n",
      " [ 9.99506560e-01  9.99007008e-01  9.98280770e-01 ...  3.99431369e-01\n",
      "   4.28495090e-01  4.56248610e-01]\n",
      " [ 9.99876632e-01  9.99531708e-01  9.99058737e-01 ...  3.68715284e-01\n",
      "   3.97364363e-01  4.26425491e-01]\n",
      " [ 1.00000000e+00  9.99733625e-01  9.99376697e-01 ...  3.51679174e-01\n",
      "   3.80759842e-01  4.09805283e-01]]\n",
      "[[ 0.0000000e+00  3.1410761e-02  6.2790520e-02 ...  6.2790461e-02\n",
      "   3.1410683e-02 -8.7422777e-08]\n",
      " [ 1.5707318e-02 -3.5832878e-04  2.8832190e-02 ...  9.6676275e-02\n",
      "   6.5364942e-02  3.3989049e-02]\n",
      " [ 3.1410761e-02  1.1489575e-02  4.4761486e-03 ...  1.1920512e-01\n",
      "   8.7959372e-02  5.6626961e-02]\n",
      " ...\n",
      " [ 9.9950653e-01  9.9900675e-01  9.9828047e-01 ...  3.9943126e-01\n",
      "   4.2849469e-01  4.5624840e-01]\n",
      " [ 9.9987662e-01  9.9953181e-01  9.9905837e-01 ...  3.6871558e-01\n",
      "   3.9736420e-01  4.2642534e-01]\n",
      " [ 1.0000000e+00  9.9973363e-01  9.9937665e-01 ...  3.5167968e-01\n",
      "   3.8075975e-01  4.0980583e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(table[0])\n",
    "print(uT.numpy()[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
